{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataVisualizationled196.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMzev4YJxzhtjE8cXYNomfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gndede/python/blob/main/DataVisualizationled196.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSznMJNQwe-R"
      },
      "source": [
        "#Predicting Pittsburgh Bike-share Rides\n",
        "#This example takes a data set from a Pittsburgh bike-share company and shows the strong correlation between the maximum \n",
        "#temperature and the number of rides taken for a given day. The data set contains other variables, such as month or holiday, \n",
        "#so feel free to modify the code to explore the data further!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlRuN7UxxEE1"
      },
      "source": [
        "#Load the dataset\n",
        "#First we load the data set from a remote location."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnBsyiPzxBv2"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import io\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from js import fetch\n",
        "import json\n",
        "res = await fetch('https://jupyterlite.anaconda.cloud/b0df9a1c-3954-4c78-96e6-07ab473bea1a/files/bikeshare.csv')\n",
        "csv_data = await res.text()\n",
        "\n",
        "#Load the dataset\n",
        "#First we load the data set from a remote location.\n",
        "df = pd.read_csv(io.StringIO(csv_data))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz_38bi7yuD_"
      },
      "source": [
        "##Visualize dataframe\n",
        "from js import fetch\n",
        "import json\n",
        "res = await fetch('https://jupyterlite.anaconda.cloud/b0df9a1c-3954-4c78-96e6-07ab473bea1a/files/bikeshare.csv')\n",
        "csv_data = await res.text()\n",
        "\n",
        " #Visualize temperature and number of rides\n",
        "df_temp_rides = df.set_index('Date')[['Max Temp','n_rides']]\n",
        "fig, axs = plt.subplots(2, 1, figsize=(18,6))\n",
        "plot_temp = axs[0]\n",
        "plot_rides = axs[1]\n",
        "plot_temp.set_xlabel('Day')\n",
        "plot_temp.set_ylabel('Max Temperature')\n",
        "plot_temp.plot(df_temp_rides['Max Temp'])\n",
        "plot_rides.set_xlabel('Day')\n",
        "plot_rides.set_ylabel('Number of Rides')\n",
        "plot_rides.plot(df_temp_rides['n_rides'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBihS4juzao_"
      },
      "source": [
        "#Grid search with a gradient boosting regressor\n",
        "#Lastly, we apply a grid search using a gradient boosting regressor to see how \n",
        "#strongly each variable contributes to the n_rides prediction. We take 20% of \n",
        "#the dataset to train and then validate the estimator scores returned from the grid search.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX0c1kLdzu4D"
      },
      "source": [
        "X = pd.get_dummies(df, columns=['Month'], drop_first=True).drop(['Date','n_rides'], axis='columns')\n",
        "y = df['n_rides']\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TuA8bpwz1k6"
      },
      "source": [
        "model = make_pipeline(MinMaxScaler(), GradientBoostingRegressor())\n",
        "params = {'gradientboostingregressor__max_depth': range(3, 20)}\n",
        "grid = GridSearchCV(model, params, cv=5)\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "grid.best_estimator_.score(X_valid, y_valid)\n",
        "\n",
        "grid.best_params_\n",
        "\n",
        "# Visualize the feature importances\n",
        "\n",
        "#Lastly, we extract the feature importances from the best estimator discovered by the grid search. \n",
        "#We can see that the best estimator also ranks maximum temperator as the most important feature.\n",
        "\n",
        "variances = grid.best_estimator_.named_steps['gradientboostingregressor'].feature_importances_\n",
        "series = pd.Series(variances, index=X.columns, name='Explained Variances')\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.bar(series.index,series)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwdpzEh5yKyY"
      },
      "source": [
        "#pip install fetch"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}