{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOaSOG+kpxwv2Mc5J7R8DWp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gndede/python/blob/main/TitanicData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rCEKNCFDGJT"
      },
      "source": [
        "#http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf\n",
        "# linear algebra\n",
        "import numpy as np\n",
        "# data processing\n",
        "import pandas as pd\n",
        "\n",
        "# data visualization\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import style\n",
        "\n",
        "# Algorithms\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXPKKvO7C-Iq"
      },
      "source": [
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "train_df = pd.read_csv(\"/content/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYJVf-QSuamS"
      },
      "source": [
        "The training-set has 891 examples and 11 features + the target variable (survived). 2 of the features are floats, 5 are integers and 5 are objects. Below I have listed the features with a short description:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2xvDrubECGo"
      },
      "source": [
        "'''survival:    Survival\n",
        "PassengerId: Unique Id of a passenger.\n",
        "pclass:    Ticket class\n",
        "sex:    Sex\n",
        "Age:    Age in years\n",
        "sibsp:    # of siblings / spouses aboard the Titanic\n",
        "parch:    # of parents / children aboard the Titanic\n",
        "ticket:    Ticket number\n",
        "fare:    Passenger fare\n",
        "cabin:    Cabin number\n",
        "embarked:    Port of Embarkation'''\n",
        "train_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAyUzeRcEIbt"
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-RDkJEHEidM"
      },
      "source": [
        "train_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjIaTzspEoCQ"
      },
      "source": [
        "'''\n",
        "From the table above, we can note a few things.\n",
        "First of all, that we need to convert a lot of features\n",
        "into numeric ones later on, so that the machine learning\n",
        "algorithms can process them.\n",
        "Furthermore, we can see that the features have widely different ranges,\n",
        "that we will need to convert into roughly the same scale.\n",
        "We can also spot some more features, that contain\n",
        "missing values (NaN = not a number), that we need to deal with.\n",
        "'''\n",
        "#Let’s take a more detailed look at what data is actually missing:\n",
        "total = train_df.isnull().sum().sort_values(ascending=False)\n",
        "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
        "\n",
        "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
        "missing_data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoQfhBA71Kyf"
      },
      "source": [
        "**Embarked**\n",
        "The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the ‘Age’ feature, which has 177 missing values. The ‘Cabin’ feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77% of it are missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz2pE9JCE_ve"
      },
      "source": [
        "train_df.columns.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SaVJ1rZFNJ5"
      },
      "source": [
        "#To me it would make sense if everything except ‘PassengerId’, ‘Ticket’\n",
        "#and ‘Name’ would be correlated with a high survival rate.\n",
        "survived = 'survived'\n",
        "not_survived = 'not survived'\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
        "women = train_df[train_df['Sex']=='female']\n",
        "men = train_df[train_df['Sex']=='male']\n",
        "\n",
        "ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
        "ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
        "ax.legend()\n",
        "ax.set_title('Female/Survived or Not')\n",
        "\n",
        "ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
        "ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\n",
        "ax.legend()\n",
        "\n",
        "_ = ax.set_title('Male/Survide or Not')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHAVz9zq3gnE"
      },
      "source": [
        "You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD2LL-c3F1XZ"
      },
      "source": [
        "#Embarked, Pclass and Sex:\n",
        "FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\n",
        "FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\n",
        "FacetGrid.add_legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PZJ6-K-HRbB"
      },
      "source": [
        "sns.barplot(x='Pclass', y='Survived', data=train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfXy4vv4HZFs"
      },
      "source": [
        "#Pclass is contributing to a persons chance of survival,\n",
        "#especially if this person is in class 1.\n",
        "#We will create another pclass plot below.\n",
        "grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n",
        "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
        "grid.add_legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWkiBFqoIXDt"
      },
      "source": [
        "The plot above confirms our assumption about passenger class 1, but we can also spot a high probability that a person in passenger class 3 will not survive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cScWx3-KJF7_"
      },
      "source": [
        "'''SibSp and Parch would make more sense as a combined feature,\n",
        "that shows the total number of relatives, a person has on the Titanic.\n",
        "We will create it below and also a feature that sows if someone is not alone.'''\n",
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n",
        "    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n",
        "    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n",
        "    dataset['not_alone'] = dataset['not_alone'].astype(int)\n",
        "train_df['not_alone'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKys3sLPLIVe"
      },
      "source": [
        "axes = sns.factorplot('relatives','Survived',\n",
        "                      data=train_df, aspect = 2.5, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVEUDq28LcZn"
      },
      "source": [
        "Here we can see that you had a high probabilty of survival with 1 to 3 realitves, but a lower one if you had less than 1 or more than 3 (except for some cases with 6 relatives).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndudpPU4Lexd"
      },
      "source": [
        "#drop ‘PassengerId’ from the train set, because\n",
        "#it does not contribute to a persons survival probability.\n",
        "train_df = train_df.drop(['PassengerId'], axis=1)\n",
        "#train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y1HZrN1Mmix"
      },
      "source": [
        "**Cabin:**\n",
        "As a reminder, we have to deal with Cabin (687), Embarked (2) and Age (177). First I thought, we have to delete the ‘Cabin’ variable but then I found something interesting. A cabin number looks like ‘C123’ and the letter refers to the deck. Therefore we’re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero. In the picture below you can see the actual decks of the titanic, ranging from A to G."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HMxinlgMsYd"
      },
      "source": [
        "import re\n",
        "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
        "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
        "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
        "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
        "    dataset['Deck'] = dataset['Deck'].astype(int)\n",
        "# we can now drop the cabin feature\n",
        "train_df = train_df.drop(['Cabin'], axis=1)\n",
        "test_df = test_df.drop(['Cabin'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4V3qVxfNFri"
      },
      "source": [
        "**Age:**\n",
        "Now we can tackle the issue with the age features missing values.\n",
        "I will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_N5aeDPNJ4x"
      },
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    mean = train_df[\"Age\"].mean()\n",
        "    std = test_df[\"Age\"].std()\n",
        "    is_null = dataset[\"Age\"].isnull().sum()\n",
        "\n",
        "    # compute random numbers between the mean, std and is_null\n",
        "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
        "\n",
        "    # fill NaN values in Age column with random values generated\n",
        "    age_slice = dataset[\"Age\"].copy()\n",
        "    age_slice[np.isnan(age_slice)] = rand_age\n",
        "    dataset[\"Age\"] = age_slice\n",
        "    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\n",
        "train_df[\"Age\"].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nacS7bXNzty"
      },
      "source": [
        "#Embarked:\n",
        "#Since the Embarked feature has only 2 missing values,\n",
        "#we will just fill these with the most common one.\n",
        "train_df['Embarked'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N18X1wpXOQo7"
      },
      "source": [
        "common_value = 'S'\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8d_OtXCOZym"
      },
      "source": [
        "train_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNZWHcHuOpvO"
      },
      "source": [
        "Above you can see that ‘Fare’ is a float and we have to deal with 4 categorical features:\n",
        "Name, Sex, Ticket and Embarked. Lets investigate and transfrom one after another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9un_LoIOvjL"
      },
      "source": [
        "#Fare:\n",
        "#Converting “Fare” from float to int64, using the “astype()” function pandas provides:\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Fare'] = dataset['Fare'].fillna(0)\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxch-KroYoRO"
      },
      "source": [
        "#Name:\n",
        "#We will use the Name feature to extract the Titles\n",
        "#from the Name, so that we can build a new feature out of that.\n",
        "data = [train_df, test_df]\n",
        "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "\n",
        "for dataset in data:\n",
        "    # extract titles\n",
        "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    # replace titles with a more common title or as Rare\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
        "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    # convert titles into numbers\n",
        "    dataset['Title'] = dataset['Title'].map(titles)\n",
        "    # filling NaN with 0, to get safe\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "train_df = train_df.drop(['Name'], axis=1)\n",
        "test_df = test_df.drop(['Name'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcYLUXN7Y0K3"
      },
      "source": [
        "#Convert 'Sex' feature into numeric\n",
        "genders = {\"male\": 0, \"female\": 1}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Sex'] = dataset['Sex'].map(genders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3ptIddAZFrP"
      },
      "source": [
        "#Ticket:\n",
        "train_df['Ticket'].describe()\n",
        "#Since the Ticket attribute has 681 unique tickets,\n",
        "#it will be a bit tricky to convert them into useful categories.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpLEFegyaIl7"
      },
      "source": [
        "#So we will drop it from the dataset.\n",
        "train_df = train_df.drop(['Ticket'], axis=1)\n",
        "test_df = test_df.drop(['Ticket'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8KxKDlbay3m"
      },
      "source": [
        "#Convert \"Embarked\" feature into numeric\n",
        "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].map(ports)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEGZFNWTcA0T"
      },
      "source": [
        "# **Creating Categories:**\n",
        "We will now create categories within the following features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5JfqKg_cFex"
      },
      "source": [
        "#Age:\n",
        "'''Now we need to convert the ‘age’ feature.\n",
        "First we will convert it from float into integer.\n",
        "Then we will create the new ‘AgeGroup” variable,\n",
        "by categorizing every age into a group.\n",
        "Note that it is important to place attention\n",
        "on how you form these groups, since you don’t\n",
        "want for example that 80% of your data falls into group 1.'''\n",
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n",
        "    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n",
        "    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n",
        "    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n",
        "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n",
        "    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n",
        "#print(dataset)\n",
        "# let's see how it's distributed train_df['Age'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp35VJRydPi8"
      },
      "source": [
        "#train_df['Age']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syOgpxIRBLuj"
      },
      "source": [
        "'''Fare:\n",
        "Fare:\n",
        "For the ‘Fare’ feature, we need to do the same as with the ‘Age’ feature.\n",
        "But it isn’t that easy, because if we cut the range of the fare values\n",
        "into a few equally big categories, 80% of the values would fall into the\n",
        "first category. Fortunately, we can use sklearn “qcut()” function,\n",
        "that we can use to see, how we can form the categories.'''\n",
        "train_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6434WooLAxu"
      },
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n",
        "    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n",
        "    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zcpNQ3TLJZc"
      },
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age_Class']= dataset['Age']* dataset['Pclass']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoAx1vylLPP9"
      },
      "source": [
        "for dataset in data:\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n",
        "# Let's take a last look at the training set, before we start training the models.\n",
        "train_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhOZzDZnEM4s"
      },
      "source": [
        "data = [train_df, test_df]\n",
        "#data\n",
        "for dataset in data:\n",
        "  dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
        "  dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "  #dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare']) <= 31), 'Fare'] = 2\n",
        "  dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "  #dataset.loc[dataset['Fare'] > 31) & (dataset['Fare']) <= 99), 'Fare'] = 3\n",
        "  dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n",
        "  #dataset.loc[dataset['Fare'] > 99) & (dataset['Fare']) <= 250), 'Fare'] = 4\n",
        "  dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n",
        "  dataset.loc[dataset['Fare'] > 250,  'Fare'] = 5\n",
        "  dataset['Fare'] = dataset['Fare'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGJAIDWxHryt"
      },
      "source": [
        "#Creating new features.\n",
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age_Class'] = dataset['Age'] * dataset['Pclass']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj7HRdoqIT9R"
      },
      "source": [
        "#Fare per person\n",
        "'''for dataset in data:\n",
        "  dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
        "  dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n",
        "\n",
        "  #Let's take a look at the training set, before we start traiing the model\n",
        "  train_df.head(10)'''\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n",
        "# Let's take a last look at the training set, before we start training the models.\n",
        "train_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9LbEAOQMY3R"
      },
      "source": [
        "**Building the Machine Learning Models**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tHoBpZWMolP"
      },
      "source": [
        "X_train = train_df.drop(\"Survived\", axis=1)\n",
        "Y_train = train_df[\"Survived\"]\n",
        "X_test = test_df.drop(\"PassengerId\", axis=1).copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDmFgnb9NbLF"
      },
      "source": [
        "**Stochastic Gradient Descent (SGD)**\n",
        "Stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs. It's an inexact but powerful technique. Stochastic gradient descent is widely used in machine learning applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdAQvgNTN7GV"
      },
      "source": [
        "#Stochastic gradient descent classier\n",
        "sgd = linear_model .SGDClassifier(max_iter=5, tol=None)\n",
        "sgd.fit(X_train, Y_train)\n",
        "Y_pred = sgd.predict(X_test)\n",
        "\n",
        "sgd.score(X_train, Y_train)\n",
        "acc_sgd = round(sgd.score(X_train, Y_train)* 100,2)\n",
        "acc_sgd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQPm7PkeO-2r"
      },
      "source": [
        "#Random Forest Classifier\n",
        "random_forest = RandomForestClassifier(n_estimators=100)\n",
        "random_forest.fit(X_train, Y_train)\n",
        "\n",
        "Y_prediction = random_forest.predict(X_train)\n",
        "\n",
        "random_forest.score(X_train, Y_train)\n",
        "acc_random_forest = round(random_forest.score(X_train, Y_train)*100,2)\n",
        "acc_random_forest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL8VvyVfPzfc"
      },
      "source": [
        "#Logistic Regeression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = logreg.predict(X_train)\n",
        "acc_log = round(logreg.score(X_train, Y_train)*100, 2)\n",
        "acc_log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AN-nSgJQ8o9"
      },
      "source": [
        "#KNN K Nearest Neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "knn.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = knn.predict(X_train)\n",
        "\n",
        "acc_knn = round(knn.score(X_train, Y_train)*100, 2)\n",
        "acc_knn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R33icqq5Sxq7"
      },
      "source": [
        "#Gaussian Naive Bayes\n",
        "gaussian = GaussianNB()\n",
        "gaussian.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = gaussian.predict(X_train)\n",
        "\n",
        "acc_gaussian = round(gaussian.score(X_train, Y_train)*100, 2)\n",
        "acc_gaussian"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPO3av5DVlTk"
      },
      "source": [
        "#Decicion Tree Classifier\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = decision_tree.predict(X_train)\n",
        "\n",
        "acc_decision_tree = round(decision_tree.score(X_train, Y_train)*100, 2)\n",
        "acc_decision_tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmkIH3VW8YK"
      },
      "source": [
        "#Linear Support Vector Machine\n",
        "linear_svc = LinearSVC()\n",
        "linear_svc.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = linear_svc.predict(X_train)\n",
        "\n",
        "acc_linear_svc = round(linear_svc.score(X_train, Y_train)*100,2)\n",
        "acc_linear_svc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYPT1NNtYXUk"
      },
      "source": [
        "#Perceptron Classifier\n",
        "\n",
        "'''A Perceptron is an algorithm for supervised learning\n",
        "of binary classifiers. This algorithm enables neurons\n",
        "to learn and processes elements in the training set\n",
        "one at a time. There are two types of Perceptrons:\n",
        "Single layer and Multilayer. Single layer Perceptrons can\n",
        "learn only linearly separable patterns.'''\n",
        "perceptron = Perceptron(max_iter=5)\n",
        "perceptron.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = perceptron.predict(X_train)\n",
        "\n",
        "acc_perceptron = round(perceptron.score(X_train, Y_train)*100, 2)\n",
        "acc_perceptron"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeLPWS-VeU7q"
      },
      "source": [
        "results = pd.DataFrame({\n",
        "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',\n",
        "              'Random Forest', 'Naive Bayes', 'Perceptron',\n",
        "              'Stochastic Gradient Decent',\n",
        "              'Decision Tree'],\n",
        "    'Score': [acc_linear_svc, acc_knn, acc_log,\n",
        "              acc_random_forest, acc_gaussian, acc_perceptron,\n",
        "              acc_sgd, acc_decision_tree]})\n",
        "result_df = results.sort_values(by='Score', ascending=False)\n",
        "result_df = result_df.set_index('Score')\n",
        "result_df.head(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGzgE9LHY3F-"
      },
      "source": [
        "'''#Cross Validation\n",
        "results = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistics Regression', 'Random Forest', 'Naive Bayes','Stochastic Gradient Decent', 'Decision Tree', 'Perceptron'],\n",
        "                        'Score': [acc_linear_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_percetron, acc_sgd, acc_decision] })\n",
        "\n",
        "result_df = results.sort_values(by='Score', ascending=False)\n",
        "result_df = result_df.set_index('Score')\n",
        "result_df.head()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6URhe_Ld9Vm"
      },
      "source": [
        "'''As we can see, the Random Forest classifier goes on the first place.\n",
        "But first, let us check, how random-forest performs, when we use cross validation.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA_06uRfdmP6"
      },
      "source": [
        "K-FOLD CROSS VALIDATION:\n",
        "\n"
      ]
    }
  ]
}