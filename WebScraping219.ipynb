{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraping219.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjobCNDLaCaeJUosO8Pwaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gndede/python/blob/main/WebScraping219.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkNwuOCiZgkd"
      },
      "source": [
        "Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvGfS-6RavFE"
      },
      "source": [
        "#In this lab exercise, we'll scrape Goodread's Best Books list:\n",
        "#link to scrap https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1 .\n",
        "#We'll walk through scraping the list pages for the book names/urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjHp5JLBbUE5"
      },
      "source": [
        "**Table of Contents¶**\n",
        "\n",
        "1: Learning Goals\n",
        "2: Exploring the Web pages and downloading them\n",
        "3: Parse the page, extract book urls\n",
        "4: Parse a book page, extract book properties\n",
        "5: Set up a pipeline for fetching and parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sUhyHTIb1Vk"
      },
      "source": [
        "**Learning Goals**\n",
        "\n",
        "Understand the structure of a web page. Use Beautiful soup to scrape content from these web pages.\n",
        "\n",
        "This lab corresponds to lectures 2, 3 and 4 and maps on to homework 1 and further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITLVdQJEcBlO"
      },
      "source": [
        "**1. Exploring the web pages and downloading them¶**\n",
        "\n",
        "We're going to see the structure of Goodread's best books list. We'll use the Developer tools in chrome, safari and firefox have similar tools available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBDxRu_l1y6G"
      },
      "source": [
        "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
        "from IPython.core.display import HTML\n",
        "def css_styling():\n",
        "    styles = open(\"../../styles/cs109.css\", \"r\").read()\n",
        "    return HTML(styles)\n",
        "css_styling()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUQ0IFhKZgGz"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn.apionly as sns\n",
        "pd.set_option('display.width', 500)\n",
        "pd.set_option('display.max_columns', 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQZVdZXuc0Aq"
      },
      "source": [
        "To fetch this page, we use the requests module. But, are we allowed to do this? Lets check:\n",
        "\n",
        "https://www.goodreads.com/robots.txt\n",
        "\n",
        "Yes we are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mWwkOlahSn"
      },
      "source": [
        "import time, requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstlZfHkanCc"
      },
      "source": [
        "URLSTART=\"https://www.goodreads.com\"\n",
        "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
        "url = URLSTART+BESTBOOKS+'1'\n",
        "print(url)\n",
        "page = requests.get(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4Hy4h6Cc_Az"
      },
      "source": [
        "We can see properties of the page. Most relevant are status_code and text. The first line tells us if the web-page was found, and if found , ok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_LBso2-dQnN"
      },
      "source": [
        "page.status_code # 200 is good"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9fABHA3dVBL"
      },
      "source": [
        "page.text[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nFRLIondZ9N"
      },
      "source": [
        "#Let us write a loop to fetch 2 pages of \"best-books\" from goodreads. \n",
        "#Notice the use of a format string. This is an example of old-style python format strings\n",
        "URLSTART=\"https://www.goodreads.com\"\n",
        "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
        "for i in range(1,3):\n",
        "    bookpage=str(i)\n",
        "    stuff=requests.get(URLSTART+BESTBOOKS+bookpage)\n",
        "    filetowrite=\"files/page\"+ '%02d' % i + \".html\"\n",
        "    print(\"FTW\", filetowrite)\n",
        "    fd=open(filetowrite,\"w\")\n",
        "    fd.write(stuff.text)\n",
        "    fd.close()\n",
        "    time.sleep(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ecokdA3d2G9"
      },
      "source": [
        "**2. Parse the page, extract book urls**\n",
        "\n",
        "Notice how we do file input-output, and use beautiful soup in the code below. The with construct ensures that the file being read is closed, something we do explicitly for the file being written. We look for the elements with class bookTitle, extract the urls, and write them into a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4o9RuCIeIRQ"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "bookdict={}\n",
        "for i in range(1,3):\n",
        "    books=[]\n",
        "    stri = '%02d' % i\n",
        "    filetoread=\"files/page\"+ stri + '.html'\n",
        "    print(\"FTW\", filetoread)\n",
        "    with open(filetoread) as fdr:\n",
        "        data = fdr.read()\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "    for e in soup.select('.bookTitle'):\n",
        "        books.append(e['href'])\n",
        "    print(books[:10])\n",
        "    bookdict[stri]=books\n",
        "    fd=open(\"files/list\"+stri+\".txt\",\"w\")\n",
        "    fd.write(\"\\n\".join(books))\n",
        "    fd.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lonRJf2heULB"
      },
      "source": [
        "#Here is George Orwell's 1984\n",
        "bookdict['02'][0]\n",
        "\n",
        "#Lets go look at the first URLs on both pages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIsR8VLze5fm"
      },
      "source": [
        "**3. Parse a book page, extract book properties**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4xauptnfGKF"
      },
      "source": [
        "#Ok so now lets dive in and get one of these these files and parse them.\n",
        "furl=URLSTART+bookdict['02'][0]\n",
        "furl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mty6ZM6fRuf"
      },
      "source": [
        "fstuff=requests.get(furl)\n",
        "print(fstuff.status_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNaRflREfW5N"
      },
      "source": [
        "d=BeautifulSoup(fstuff.text, 'html.parser')\n",
        "d.select(\"meta[property='og:title']\")[0]['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy7FaYNvfd4f"
      },
      "source": [
        "#Lets get everything we want...\n",
        "d=BeautifulSoup(fstuff.text, 'html.parser')\n",
        "print(\n",
        "\"title\", d.select_one(\"meta[property='og:title']\")['content'],\"\\n\",\n",
        "\"isbn\", d.select(\"meta[property='books:isbn']\")[0]['content'],\"\\n\",\n",
        "\"type\", d.select(\"meta[property='og:type']\")[0]['content'],\"\\n\",\n",
        "\"author\", d.select(\"meta[property='books:author']\")[0]['content'],\"\\n\",\n",
        "\"average rating\", d.select_one(\"span.average\").text,\"\\n\",\n",
        "\"ratingCount\", d.select(\"meta[itemprop='ratingCount']\")[0][\"content\"],\"\\n\",\n",
        "\"reviewCount\", d.select_one(\"span.count\")[\"title\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfvU2B6EfqpK"
      },
      "source": [
        "Ok, now that we know what to do, lets wrap our fetching into a proper script. So that we dont overwhelm their servers, we will only fetch 5 from each page, but you get the idea...\n",
        "\n",
        "\n",
        "We'll segue of a bit to explore new style format strings. See https://pyformat.info for more info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylYFpuYCfwSI"
      },
      "source": [
        "\"list{:0>2}.txt\".format(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEwoy5Ydf0qg"
      },
      "source": [
        "a = \"4\"\n",
        "b = 4\n",
        "class Four:\n",
        "    def __str__(self):\n",
        "        return \"Fourteen\"\n",
        "c=Four()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jik-Mw39hJa1"
      },
      "source": [
        "\"The lazy cat jumped over the {} and {} and {}\".format(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKuhoO6iz_-4"
      },
      "source": [
        "**4. Set up a pipeline for fetching and parsing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_hAxD5i0F0I"
      },
      "source": [
        "# Ok lets get back to the fetching process..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le3gK-gj0Mx-"
      },
      "source": [
        "fetched=[]\n",
        "for i in range(1,3):\n",
        "    with open(\"files/list{:0>2}.txt\".format(i)) as fd:\n",
        "        counter=0\n",
        "        for bookurl_line in fd:\n",
        "            if counter > 4:\n",
        "                break\n",
        "            bookurl=bookurl_line.strip()\n",
        "            stuff=requests.get(URLSTART+bookurl)\n",
        "            filetowrite=bookurl.split('/')[-1]\n",
        "            filetowrite=\"files/\"+str(i)+\"_\"+filetowrite+\".html\"\n",
        "            print(\"FTW\", filetowrite)\n",
        "            fd=open(filetowrite,\"w\", encoding='utf-8')\n",
        "            fd.write(stuff.text)\n",
        "            fd.close()\n",
        "            fetched.append(filetowrite)\n",
        "            time.sleep(2)\n",
        "            counter=counter+1\n",
        "            \n",
        "print(fetched)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zbD7CF80UbU"
      },
      "source": [
        "# Ok we are off to parse each one of the html pages we fetched. \n",
        "# We have provided the skeleton of the code and the code to parse the year, \n",
        "#since it is a bit more complex...see the difference in the screenshots above."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vT839Qb0cWa"
      },
      "source": [
        "import re\n",
        "yearre = r'\\d{4}'\n",
        "def get_year(d):\n",
        "    if d.select_one(\"nobr.greyText\"):\n",
        "        return d.select_one(\"nobr.greyText\").text.strip().split()[-1][:-1]\n",
        "    else:\n",
        "        thetext=d.select(\"div#details div.row\")[1].text.strip()\n",
        "        rowmatch=re.findall(yearre, thetext)\n",
        "        if len(rowmatch) > 0:\n",
        "            rowtext=rowmatch[0].strip()\n",
        "        else:\n",
        "            rowtext=\"NA\"\n",
        "        return rowtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mydkILIR0jwE"
      },
      "source": [
        "#Your job is to fill in the code to get the genres.\n",
        "def get_genres(d):\n",
        "    # your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ljtg9kK0uYM"
      },
      "source": [
        "listofdicts=[]\n",
        "for filetoread in fetched:\n",
        "    print(filetoread)\n",
        "    td={}\n",
        "    with open(filetoread) as fd:\n",
        "        datext = fd.read()\n",
        "    d=BeautifulSoup(datext, 'html.parser')\n",
        "    td['title']=d.select_one(\"meta[property='og:title']\")['content']\n",
        "    td['isbn']=d.select_one(\"meta[property='books:isbn']\")['content']\n",
        "    td['booktype']=d.select_one(\"meta[property='og:type']\")['content']\n",
        "    td['author']=d.select_one(\"meta[property='books:author']\")['content']\n",
        "    td['rating']=d.select_one(\"span.average\").text\n",
        "    td['ratingCount']=d.select_one(\"meta[itemprop='ratingCount']\")[\"content\"]\n",
        "    td['reviewCount']=d.select_one(\"span.count\")[\"title\"]\n",
        "    td['year'] = get_year(d)\n",
        "    td['file']=filetoread\n",
        "    glist = get_genres(d)\n",
        "    td['genres']=\"|\".join(glist)\n",
        "    listofdicts.append(td)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCa2xhQK05pj"
      },
      "source": [
        "listofdicts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaMv_J_z0-pj"
      },
      "source": [
        "#Finally lets write all this stuff into a csv file which we will use to do analysis.\n",
        "df = pd.DataFrame.from_records(listofdicts)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdEr_iNB1Dkx"
      },
      "source": [
        "df.to_csv(\"files/meta.csv\", index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}