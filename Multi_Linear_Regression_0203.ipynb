{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi Linear Regression-0203.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMGThhU5Q0J0R8rXIQLsj+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gndede/python/blob/main/Multi_Linear_Regression_0203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL-YteKyDLZL"
      },
      "source": [
        "**Multiple Linear Regression**\n",
        "That also similarly applies to Multiple Linear Regression. The goal is still to fit a line that best shows the relationship between an independent variable and the target. The difference is that in Multiple Linear Regression, we have to deal with at least 2 features or independent variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2NH-p6CDkAj"
      },
      "source": [
        "Notice that there are multiple features or independent variables (R&D Spend, Administration, Marketing Spend, State). Again, the goal here is to reveal or discover a relationship between the independent variables and the target (Profit). Also notice that under the column ‘State’, the data is in text (not numbers). You’ll see New York, California, and Florida instead of numbers. How do you deal with this kind of data?\n",
        "\n",
        "One convenient way to do that is by transforming categorical data (New York, California, Florida) into numerical data. We can accomplish this if we use the following lines of code: from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "X[:, 3] = labelencoder.fit_transform(X[:, 3]) #Note this\n",
        "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
        "X = onehotencoder.fit_transform(X).toarray() Pay attention to X[:, 3] =\n",
        "labelencoder.fit_transform(X[:, 3]) What we did there is to transform the data in the fourth column (State). It’s number 3 because Python indexing starts at zero (0). The goal was to transform categorical variables data into something we can work on. To do this, we’ll create “dummy variables” which take the values of 0 or 1. In other words, they indicate the presence or absence of something.\n",
        "\n",
        "For example, we have the following data with categorical variables: 3.5, New York 2.0, California 6.7, Florida If we use dummy variables, the above data will be transformed into this: \n",
        "3.5, 1, 0, 0\n",
        "3.5, 1, 0, 0\n",
        "2.0, 0, 1, 0\n",
        "6.7, 0, 0, 1\n",
        "\n",
        "Notice that the column for State became equivalent to 3 columns:\n",
        "    New York   California    Florida\n",
        "3.5   1          0            0\n",
        "2.0   0          1            0\n",
        "6.7   0          0            1\n",
        "\n",
        "As mentioned earlier, dummy variables indicate the presence or absence of something. They are commonly used as “substitute variables” so we can do a quantitative analysis on qualitative data. From the new table above we can quickly see that 3.5 is for New York (1 New York, 0 California, and 0 Florida). It’s a convenient way of representing categories into numeric values.\n",
        "\n",
        "However, there’s this so-called “dummy variable trap” wherein there’s an extra variable that could have been removed because it can be predicted from the others. In our example above, notice that when the columns for New York and California are zero (0), automatically you’ll know it’s Florida. You can already know which State it is even with just the 2 variable. Continuing with our work on 50_Startups.csv, we can avoid the dummy variable\n",
        "trap by including this in our code: X = X[:, 1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD3-UfofDEha"
      },
      "source": [
        "#For example, let’s look at a dataset about 50 startups ((50_Startups.csv):\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/50_Startups.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, 4].values #Let’s look at the data: dataset.head()\n",
        "#dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvJoOnOhGuRs"
      },
      "source": [
        "#We therefore transform categorical variables into numeric ones (dummy variables):\n",
        "# Encoding categorical data\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "dataset = py.read_csv(\"/content/50_Startups.csv\")\n",
        "X = dataset.iloc[:, :].values\n",
        "\n",
        "#from sklearn.preprocessing import LabelEncoder\n",
        "labelEncoder = LabelEncoder()\n",
        "X[:, 3] = labelEncoder.fit_transform(X[:, 3])\n",
        "\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
        "#onehotencoder = OneHotEncoder(categorical_features = [3])\n",
        "#X = onehotencoder.fit_transform(X).toarray() # Avoiding the Dummy Variable Trap\n",
        "X = X[:, 1:]\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFJc71PMKdS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
        "random_state = 0)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzqfAEVZJO9E"
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = regressor.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyV4QOYoH_EX"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "label_encoder_x_1 = LabelEncoder()\n",
        "X[: , 2] = label_encoder_x_1.fit_transform(X[:,2])\n",
        "transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"OneHot\",        # Just a name\n",
        "         OneHotEncoder(), # The transformer class\n",
        "         [1]              # The column(s) to be applied on.\n",
        "         )\n",
        "    ],\n",
        "    remainder='passthrough' # donot apply anything to the remaining columns\n",
        ")\n",
        "X = transformer.fit_transform(X.tolist())\n",
        "X = X.astype('float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSzgftL-I2Sr"
      },
      "source": [
        "Notice that there are no categorical variables (New York, California, Florida) and we’ve removed the “redundant variable” to avoid the dummy variable trap.\n",
        "\n",
        "Now we’re all set to dividing the dataset into Training Set and Test Set. We can do this with the following lines of code: **from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)** 80% Training Set, 20% Test Set. Next step is we can then create a regressor and “fit the line” (and use that line on Test Set): **from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X_train, y_train)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esdzj1OFJug3"
      },
      "source": [
        "However, is that all there is? Are all the variables (R&D Spend, Administration, Marketing Spend, State) responsible for the target (Profit). Many data analystsperform additional steps to create better models and predictors. They might be doing Backward Elimination (e.g. eliminating variables one by one until there’s one or two left) so we’ll know which of the variables is making the biggest contribution to our results (and therefore more accurate predictions).\n",
        "There are other ways of making the making the model yield more accurate\n",
        "predictions. It depends on your objectives (perhaps you want to use all the data variables) and resources (not just money and computational power, but also time constraints)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HnvUyCjKajq"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as ply\n",
        "import pandas as py\n",
        "\n",
        "dataset = py.read_csv(\"/content/50_Startups.csv\")\n",
        "X = dataset.iloc[:, :].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelEncoder = LabelEncoder()\n",
        "X[:, 3] = labelEncoder.fit_transform(X[:, 3])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}